{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Practical 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "    \n",
    "raw_text = doc.xpath('//content/text()')\n",
    "raw_label = doc.xpath('//head/keywords/text()')\n",
    "\n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_sentences = []\n",
    "talknum = len(raw_text)\n",
    "\n",
    "for i in range(talknum):\n",
    "    temp = re.sub(r'\\([^)]*\\)', '', raw_text[i])\n",
    "    temp = re.sub(r'\\n', '', raw_text[i])\n",
    "    temp = temp.split('.')\n",
    "    talk_sentences.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation\n"
     ]
    }
   ],
   "source": [
    "print(talk_sentences[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "talk_sentence_word = []\n",
    "\n",
    "for talk in talk_sentences:\n",
    "    temp = []\n",
    "    for sent in talk:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent.lower()).split()\n",
    "        temp.append(tokens)#\n",
    "    talk_sentence_word.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'me',\n",
       " 'the',\n",
       " 'real',\n",
       " 'real',\n",
       " 'solution',\n",
       " 'to',\n",
       " 'quality',\n",
       " 'growth',\n",
       " 'is',\n",
       " 'figuring',\n",
       " 'out',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'two',\n",
       " 'activities',\n",
       " 'exploration',\n",
       " 'and',\n",
       " 'exploitation']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_sentence_word[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "\n",
    "for talk in talk_sentence_word:\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            if word in freq:\n",
    "                freq[word] += 1\n",
    "            else:\n",
    "                freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts_ted_top1000 = []\n",
    "words_top_ted = []\n",
    "\n",
    "MC = Counter(freq).most_common(1000)\n",
    "\n",
    "for word, count in MC:\n",
    "    counts_ted_top1000.append(count)\n",
    "    words_top_ted.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_sentence_word_nostop = talk_sentence_word\n",
    "\n",
    "for stop in words_top_ted[:200]:\n",
    "    for talk in talk_sentence_word_nostop:\n",
    "        for sent in talk:\n",
    "            for word in sent:\n",
    "                if word == stop:\n",
    "                    sent.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real',\n",
       " 'real',\n",
       " 'solution',\n",
       " 'quality',\n",
       " 'growth',\n",
       " 'figuring',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'activities',\n",
       " 'exploration',\n",
       " 'exploitation']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_sentence_word_nostop[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2085,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(talk_sentence_word_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_label = []\n",
    "\n",
    "for i in range(len(raw_label)):\n",
    "    temp = re.sub(r' ', '', raw_label[i])\n",
    "    input_label.append(temp.split(','))\n",
    "    \n",
    "#print(input_label_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talks', 'Senses', 'augmentedreality', 'brain', 'computers', 'creativity', 'cyborg', 'demo', 'design', 'engineering', 'entrepreneur', 'innovation', 'interfacedesign', 'invention', 'neuroscience', 'potential', 'prediction', 'productdesign', 'technology', 'visualizations']\n",
      "[array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  1.]), array([ 1.,  0.,  1.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 1.,  0.,  1.])]\n"
     ]
    }
   ],
   "source": [
    "labels_binary = []\n",
    "\n",
    "for i in range(len(input_label)):\n",
    "    temp = np.zeros(3)\n",
    "    if 'technology' in '~'.join(input_label[i]):\n",
    "        temp[0] = 1\n",
    "    if 'entertainment' in '~'.join(input_label[i]):\n",
    "        temp[1] = 1\n",
    "    if 'design' in '~'.join(input_label[i]):\n",
    "        temp[2] = 1\n",
    "    labels_binary.append(temp)\n",
    "        \n",
    "print(input_label[9])\n",
    "print(labels_binary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = list(zip(talk_sentence_word_nostop, labels_binary))\n",
    "random.shuffle(temp)\n",
    "talk_sentence_word_nostop_shuffle, labels_binary_shuffle = zip(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "labels_onehot = []\n",
    "for i in range(len(input_label)):\n",
    "    temp = np.zeros(8)\n",
    "    temp[labels_binary[i][0]*4+labels_binary[i][1]*2+labels_binary[i][2]*1] = 1\n",
    "    labels_onehot.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "labels_onehot_shuffle = []\n",
    "for i in range(len(input_label)):\n",
    "    temp = np.zeros(8)\n",
    "    temp[labels_binary_shuffle[i][0]*4+labels_binary_shuffle[i][1]*2+labels_binary_shuffle[i][2]*1] = 1\n",
    "    labels_onehot_shuffle.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = talk_sentence_word_nostop[:1835]\n",
    "test_data = talk_sentence_word_nostop[1835:]\n",
    "\n",
    "train_labels = labels_binary[:1835]\n",
    "test_labels = labels_binary[1835:]\n",
    "\n",
    "train_labels_onehot = labels_onehot[:1835]\n",
    "test_labels_onehot = labels_onehot[1835:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_shuffle = talk_sentence_word_nostop_shuffle[:1835]\n",
    "test_data_shuffle = talk_sentence_word_nostop_shuffle[1835:]\n",
    "\n",
    "train_labels_shuffle = labels_binary_shuffle[:1835]\n",
    "test_labels_shuffle = labels_binary_shuffle[1835:]\n",
    "\n",
    "train_labels_onehot_shuffle = labels_onehot_shuffle[:1835]\n",
    "test_labels_onehot_shuffle = labels_onehot_shuffle[1835:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2085,)\n",
      "(2085, 3)\n",
      "(2085, 8)\n",
      "(1835,)\n",
      "(1835, 3)\n",
      "(1835, 8)\n",
      "(250,)\n",
      "(250, 3)\n",
      "(250, 8)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(talk_sentence_word_nostop))\n",
    "print(np.shape(labels_binary))\n",
    "print(np.shape(labels_onehot))\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(train_labels_onehot))\n",
    "print(np.shape(test_data))\n",
    "print(np.shape(test_labels))\n",
    "print(np.shape(test_labels_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2085,)\n",
      "(2085, 3)\n",
      "(2085, 8)\n",
      "(1835,)\n",
      "(1835, 3)\n",
      "(1835, 8)\n",
      "(250,)\n",
      "(250, 3)\n",
      "(250, 8)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(talk_sentence_word_nostop_shuffle))\n",
    "print(np.shape(labels_binary_shuffle))\n",
    "print(np.shape(labels_onehot_shuffle))\n",
    "print(np.shape(train_data_shuffle))\n",
    "print(np.shape(train_labels_shuffle))\n",
    "print(np.shape(train_labels_onehot_shuffle))\n",
    "print(np.shape(test_data_shuffle))\n",
    "print(np.shape(test_labels_shuffle))\n",
    "print(np.shape(test_labels_onehot_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57493188  0.0746594   0.0719346   0.00980926  0.17711172  0.06376022\n",
      "  0.01525886  0.01253406]\n",
      "[ 0.292  0.108  0.164  0.024  0.224  0.112  0.032  0.044]\n"
     ]
    }
   ],
   "source": [
    "check = np.asarray(train_labels_onehot)\n",
    "print(check.sum(0) / check.sum())\n",
    "check = np.asarray(test_labels_onehot)\n",
    "print(check.sum(0) / check.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53787466  0.07901907  0.08174387  0.0119891   0.18692098  0.06594005\n",
      "  0.01907357  0.01743869]\n",
      "[ 0.564  0.076  0.092  0.008  0.152  0.096  0.004  0.008]\n"
     ]
    }
   ],
   "source": [
    "check = np.asarray(train_labels_onehot_shuffle)\n",
    "print(check.sum(0) / check.sum())\n",
    "check = np.asarray(test_labels_onehot_shuffle)\n",
    "print(check.sum(0) / check.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lenghts = []\n",
    "\n",
    "for talk in train_data_shuffle:\n",
    "    N = 0\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            N += 1\n",
    "    train_lenghts.append(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[973, 429, 1002, 764, 758, 615, 842, 1195, 1071, 1003]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lenghts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lenghts = []\n",
    "\n",
    "for talk in test_data_shuffle:\n",
    "    N = 0\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            N += 1\n",
    "    test_lenghts.append(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[785, 552, 309, 915, 861, 420, 771, 503, 941, 769]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lenghts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "train_talk_word = []\n",
    "\n",
    "for talk in train_data_shuffle:\n",
    "    temp = []\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            temp.append(word)\n",
    "    train_talk_word.append(temp)\n",
    "    \n",
    "print(len(train_talk_word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n"
     ]
    }
   ],
   "source": [
    "test_talk_word = []\n",
    "\n",
    "for talk in test_data_shuffle:\n",
    "    temp = []\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            temp.append(word)\n",
    "    test_talk_word.append(temp)\n",
    "    \n",
    "print(len(test_talk_word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['portuguese',\n",
       " 'arrived',\n",
       " 'latin',\n",
       " 'america',\n",
       " '500',\n",
       " 'obviously',\n",
       " 'found',\n",
       " 'amazing',\n",
       " 'tropical',\n",
       " 'forest']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_talk_word[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['reasons', 'companies', 'fail'],\n",
       " ['real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'figuring',\n",
       "  'balance',\n",
       "  'between',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'exploitation'],\n",
       " ['both', 'necessary', 'too'],\n",
       " ['consider', 'facit'],\n",
       " ['old', 'enough', 'remember']]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "N = 1\n",
    "\n",
    "for talk in train_data_shuffle:\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            if word in vocab:\n",
    "                N = N\n",
    "            else:\n",
    "                vocab[word] = N\n",
    "                N += 1\n",
    "                \n",
    "vocab[\"unknown_word\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"unknown_word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_talk_word_index = []\n",
    "\n",
    "for talk in train_talk_word:\n",
    "    temp = []\n",
    "    for word in talk:\n",
    "        if word in vocab:\n",
    "            temp.append(vocab[word])\n",
    "        else:\n",
    "            temp.append(vocab[\"unknown_word\"])\n",
    "    train_talk_word_index.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_talk_word_index = []\n",
    "\n",
    "for talk in test_talk_word:\n",
    "    temp = []\n",
    "    for word in talk:\n",
    "        if word in vocab:\n",
    "            temp.append(vocab[word])\n",
    "        else:\n",
    "            temp.append(vocab[\"unknown_word\"])\n",
    "    test_talk_word_index.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 3, 11, 12, 3, 13, 14, 13, 15, 13]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_talk_word_index[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_talk_word_index_exp = []\n",
    "\n",
    "for i in range(len(train_talk_word_index)):\n",
    "    temp = []\n",
    "    for j in range(max(train_lenghts)):\n",
    "        if j <= (train_lenghts[i]-1):\n",
    "            temp.append(train_talk_word_index[i][j])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    train_talk_word_index_exp.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_talk_word_index_exp = []\n",
    "\n",
    "for i in range(len(test_talk_word_index)):\n",
    "    temp = []\n",
    "    for j in range(max(train_lenghts)):\n",
    "        if j <= (test_lenghts[i]-1):\n",
    "            temp.append(test_talk_word_index[i][j])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    test_talk_word_index_exp.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 2221)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835, 2221)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17771,\n",
       " 44,\n",
       " 6288,\n",
       " 1426,\n",
       " 524,\n",
       " 1668,\n",
       " 967,\n",
       " 1976,\n",
       " 12727,\n",
       " 4004,\n",
       " 281,\n",
       " 7997,\n",
       " 132,\n",
       " 967,\n",
       " 6169,\n",
       " 727,\n",
       " 505,\n",
       " 1701,\n",
       " 6169,\n",
       " 1612,\n",
       " 21004,\n",
       " 936,\n",
       " 644,\n",
       " 17309,\n",
       " 2023,\n",
       " 21754,\n",
       " 5360,\n",
       " 7164,\n",
       " 6455,\n",
       " 6169,\n",
       " 0,\n",
       " 0,\n",
       " 254,\n",
       " 834,\n",
       " 634,\n",
       " 834,\n",
       " 0,\n",
       " 1279,\n",
       " 12097,\n",
       " 18,\n",
       " 192,\n",
       " 5540,\n",
       " 1855,\n",
       " 2636,\n",
       " 31582,\n",
       " 12097,\n",
       " 281,\n",
       " 2625,\n",
       " 4004,\n",
       " 335,\n",
       " 3262,\n",
       " 335,\n",
       " 4004,\n",
       " 2162,\n",
       " 4841,\n",
       " 12097,\n",
       " 106,\n",
       " 3635,\n",
       " 15161,\n",
       " 227,\n",
       " 7211,\n",
       " 2360,\n",
       " 4004,\n",
       " 6742,\n",
       " 15907,\n",
       " 7006,\n",
       " 180,\n",
       " 435,\n",
       " 439,\n",
       " 307,\n",
       " 6742,\n",
       " 2158,\n",
       " 1424,\n",
       " 2158,\n",
       " 3021,\n",
       " 772,\n",
       " 433,\n",
       " 1470,\n",
       " 435,\n",
       " 439,\n",
       " 10333,\n",
       " 307,\n",
       " 2044,\n",
       " 0,\n",
       " 65,\n",
       " 588,\n",
       " 1219,\n",
       " 2163,\n",
       " 1570,\n",
       " 3152,\n",
       " 1476,\n",
       " 2295,\n",
       " 39,\n",
       " 435,\n",
       " 18668,\n",
       " 9251,\n",
       " 179,\n",
       " 288,\n",
       " 2350,\n",
       " 3251,\n",
       " 12097,\n",
       " 1024,\n",
       " 1839,\n",
       " 15695,\n",
       " 10008,\n",
       " 6577,\n",
       " 5749,\n",
       " 4132,\n",
       " 5749,\n",
       " 435,\n",
       " 18668,\n",
       " 1602,\n",
       " 7997,\n",
       " 613,\n",
       " 274,\n",
       " 1011,\n",
       " 12743,\n",
       " 437,\n",
       " 1303,\n",
       " 65,\n",
       " 435,\n",
       " 18668,\n",
       " 4004,\n",
       " 274,\n",
       " 878,\n",
       " 746,\n",
       " 435,\n",
       " 18668,\n",
       " 878,\n",
       " 748,\n",
       " 746,\n",
       " 502,\n",
       " 16391,\n",
       " 17146,\n",
       " 10008,\n",
       " 9251,\n",
       " 17146,\n",
       " 12727,\n",
       " 9251,\n",
       " 748,\n",
       " 746,\n",
       " 543,\n",
       " 4004,\n",
       " 4132,\n",
       " 746,\n",
       " 878,\n",
       " 10008,\n",
       " 9251,\n",
       " 1168,\n",
       " 2054,\n",
       " 62,\n",
       " 12097,\n",
       " 1767,\n",
       " 6784,\n",
       " 47,\n",
       " 1424,\n",
       " 4004,\n",
       " 3199,\n",
       " 305,\n",
       " 1674,\n",
       " 9251,\n",
       " 12097,\n",
       " 6742,\n",
       " 1424,\n",
       " 1767,\n",
       " 4004,\n",
       " 1952,\n",
       " 706,\n",
       " 512,\n",
       " 108,\n",
       " 61,\n",
       " 6589,\n",
       " 4004,\n",
       " 3540,\n",
       " 797,\n",
       " 746,\n",
       " 1536,\n",
       " 92,\n",
       " 1168,\n",
       " 1695,\n",
       " 8403,\n",
       " 199,\n",
       " 6449,\n",
       " 6742,\n",
       " 36122,\n",
       " 2162,\n",
       " 17146,\n",
       " 250,\n",
       " 474,\n",
       " 10631,\n",
       " 383,\n",
       " 2708,\n",
       " 2053,\n",
       " 3217,\n",
       " 18918,\n",
       " 5623,\n",
       " 5623,\n",
       " 7717,\n",
       " 5540,\n",
       " 188,\n",
       " 2119,\n",
       " 451,\n",
       " 1345,\n",
       " 5811,\n",
       " 878,\n",
       " 1421,\n",
       " 170,\n",
       " 18668,\n",
       " 106,\n",
       " 502,\n",
       " 12703,\n",
       " 12704,\n",
       " 912,\n",
       " 9897,\n",
       " 7927,\n",
       " 24,\n",
       " 573,\n",
       " 268,\n",
       " 14053,\n",
       " 2087,\n",
       " 4004,\n",
       " 1608,\n",
       " 8110,\n",
       " 2510,\n",
       " 4767,\n",
       " 1459,\n",
       " 5927,\n",
       " 17146,\n",
       " 2702,\n",
       " 11204,\n",
       " 2087,\n",
       " 723,\n",
       " 8343,\n",
       " 988,\n",
       " 3300,\n",
       " 9904,\n",
       " 7,\n",
       " 484,\n",
       " 988,\n",
       " 4354,\n",
       " 546,\n",
       " 5777,\n",
       " 5927,\n",
       " 2702,\n",
       " 41473,\n",
       " 16,\n",
       " 1947,\n",
       " 1101,\n",
       " 2705,\n",
       " 399,\n",
       " 895,\n",
       " 34,\n",
       " 2087,\n",
       " 1973,\n",
       " 3523,\n",
       " 17146,\n",
       " 343,\n",
       " 106,\n",
       " 561,\n",
       " 12097,\n",
       " 26650,\n",
       " 173,\n",
       " 588,\n",
       " 574,\n",
       " 17146,\n",
       " 343,\n",
       " 555,\n",
       " 343,\n",
       " 574,\n",
       " 102,\n",
       " 9735,\n",
       " 1453,\n",
       " 6438,\n",
       " 1114,\n",
       " 527,\n",
       " 1132,\n",
       " 281,\n",
       " 47161,\n",
       " 456,\n",
       " 6264,\n",
       " 170,\n",
       " 8031,\n",
       " 749,\n",
       " 14553,\n",
       " 2352,\n",
       " 1968,\n",
       " 750,\n",
       " 1801,\n",
       " 8057,\n",
       " 2390,\n",
       " 2080,\n",
       " 2801,\n",
       " 2109,\n",
       " 4624,\n",
       " 988,\n",
       " 4354,\n",
       " 8070,\n",
       " 2080,\n",
       " 1132,\n",
       " 2356,\n",
       " 22319,\n",
       " 4532,\n",
       " 1770,\n",
       " 995,\n",
       " 3401,\n",
       " 62,\n",
       " 1422,\n",
       " 2308,\n",
       " 282,\n",
       " 2817,\n",
       " 16949,\n",
       " 6832,\n",
       " 1947,\n",
       " 1770,\n",
       " 17146,\n",
       " 176,\n",
       " 2452,\n",
       " 1612,\n",
       " 190,\n",
       " 79,\n",
       " 383,\n",
       " 3988,\n",
       " 15788,\n",
       " 51677,\n",
       " 1665,\n",
       " 335,\n",
       " 1770,\n",
       " 478,\n",
       " 1612,\n",
       " 1947,\n",
       " 31038,\n",
       " 17146,\n",
       " 427,\n",
       " 3015,\n",
       " 690,\n",
       " 4855,\n",
       " 834,\n",
       " 10063,\n",
       " 352,\n",
       " 8721,\n",
       " 10739,\n",
       " 170,\n",
       " 18668,\n",
       " 838,\n",
       " 1947,\n",
       " 1807,\n",
       " 706,\n",
       " 1839,\n",
       " 6143,\n",
       " 169,\n",
       " 170,\n",
       " 6455,\n",
       " 9866,\n",
       " 1971,\n",
       " 450,\n",
       " 17146,\n",
       " 12097,\n",
       " 4059,\n",
       " 1725,\n",
       " 436,\n",
       " 17146,\n",
       " 4080,\n",
       " 1223,\n",
       " 847,\n",
       " 1421,\n",
       " 170,\n",
       " 18668,\n",
       " 1839,\n",
       " 23978,\n",
       " 613,\n",
       " 15371,\n",
       " 438,\n",
       " 435,\n",
       " 439,\n",
       " 10630,\n",
       " 7006,\n",
       " 205,\n",
       " 1424,\n",
       " 22560,\n",
       " 5464,\n",
       " 3888,\n",
       " 6597,\n",
       " 3311,\n",
       " 627,\n",
       " 248,\n",
       " 4439,\n",
       " 465,\n",
       " 40,\n",
       " 16,\n",
       " 15852,\n",
       " 4855,\n",
       " 17146,\n",
       " 31,\n",
       " 41,\n",
       " 31,\n",
       " 970,\n",
       " 5,\n",
       " 494,\n",
       " 4208,\n",
       " 2521,\n",
       " 2024,\n",
       " 2547,\n",
       " 17146,\n",
       " 144,\n",
       " 1690,\n",
       " 436,\n",
       " 1882,\n",
       " 5082,\n",
       " 4080,\n",
       " 17146,\n",
       " 4696,\n",
       " 1009,\n",
       " 384,\n",
       " 5343,\n",
       " 1433,\n",
       " 62,\n",
       " 17146,\n",
       " 797,\n",
       " 494,\n",
       " 16990,\n",
       " 1668,\n",
       " 5443,\n",
       " 1580,\n",
       " 4135,\n",
       " 17146,\n",
       " 6742,\n",
       " 8525,\n",
       " 502,\n",
       " 170,\n",
       " 18668,\n",
       " 305,\n",
       " 2680,\n",
       " 706,\n",
       " 1839,\n",
       " 5388,\n",
       " 3413,\n",
       " 1612,\n",
       " 6742,\n",
       " 28228,\n",
       " 3307,\n",
       " 17146,\n",
       " 0,\n",
       " 12097,\n",
       " 466,\n",
       " 1424,\n",
       " 17146,\n",
       " 105,\n",
       " 179,\n",
       " 4004,\n",
       " 8444,\n",
       " 17146,\n",
       " 12193,\n",
       " 8228,\n",
       " 1580,\n",
       " 8228,\n",
       " 2326,\n",
       " 54,\n",
       " 3306,\n",
       " 1733,\n",
       " 4004,\n",
       " 3540,\n",
       " 12097,\n",
       " 1499,\n",
       " 13433,\n",
       " 2326,\n",
       " 509,\n",
       " 3974,\n",
       " 2428,\n",
       " 629,\n",
       " 2162,\n",
       " 9251,\n",
       " 1536,\n",
       " 3888,\n",
       " 6597,\n",
       " 3311,\n",
       " 17146,\n",
       " 9251,\n",
       " 2832,\n",
       " 653,\n",
       " 1723,\n",
       " 1996,\n",
       " 2755,\n",
       " 614,\n",
       " 437,\n",
       " 2428,\n",
       " 2162,\n",
       " 9251,\n",
       " 1266,\n",
       " 654,\n",
       " 3554,\n",
       " 9251,\n",
       " 530,\n",
       " 2162,\n",
       " 6122,\n",
       " 3251,\n",
       " 313,\n",
       " 14563,\n",
       " 19975,\n",
       " 4004,\n",
       " 1739,\n",
       " 61,\n",
       " 13280,\n",
       " 3251,\n",
       " 3199,\n",
       " 1,\n",
       " 288,\n",
       " 2350,\n",
       " 1433,\n",
       " 30173,\n",
       " 1954,\n",
       " 1697,\n",
       " 2309,\n",
       " 9251,\n",
       " 3251,\n",
       " 2976,\n",
       " 1551,\n",
       " 1571,\n",
       " 1551,\n",
       " 752,\n",
       " 1571,\n",
       " 1551,\n",
       " 24118,\n",
       " 1954,\n",
       " 1103,\n",
       " 1954,\n",
       " 7021,\n",
       " 357,\n",
       " 3698,\n",
       " 2564,\n",
       " 4135,\n",
       " 357,\n",
       " 3698,\n",
       " 1428,\n",
       " 247,\n",
       " 1954,\n",
       " 634,\n",
       " 2560,\n",
       " 1764,\n",
       " 2402,\n",
       " 1551,\n",
       " 67,\n",
       " 4004,\n",
       " 6239,\n",
       " 8207,\n",
       " 13029,\n",
       " 2162,\n",
       " 78,\n",
       " 8687,\n",
       " 530,\n",
       " 1295,\n",
       " 690,\n",
       " 2154,\n",
       " 17146,\n",
       " 1009,\n",
       " 3313,\n",
       " 56,\n",
       " 1279,\n",
       " 2162,\n",
       " 9108,\n",
       " 5345,\n",
       " 5689,\n",
       " 624,\n",
       " 3110,\n",
       " 4004,\n",
       " 2295,\n",
       " 3480,\n",
       " 2162,\n",
       " 490,\n",
       " 277,\n",
       " 277,\n",
       " 2162,\n",
       " 876,\n",
       " 13981,\n",
       " 1499,\n",
       " 478,\n",
       " 4095,\n",
       " 2419,\n",
       " 436,\n",
       " 3058,\n",
       " 1533,\n",
       " 2980,\n",
       " 4005,\n",
       " 2625,\n",
       " 1071,\n",
       " 4095,\n",
       " 2980,\n",
       " 1135,\n",
       " 2577,\n",
       " 15760,\n",
       " 26655,\n",
       " 3453,\n",
       " 2162,\n",
       " 1121,\n",
       " 4950,\n",
       " 3311,\n",
       " 456,\n",
       " 435,\n",
       " 439,\n",
       " 10630,\n",
       " 627,\n",
       " 383,\n",
       " 3114,\n",
       " 2061,\n",
       " 453,\n",
       " 436,\n",
       " 4950,\n",
       " 497,\n",
       " 435,\n",
       " 439,\n",
       " 10630,\n",
       " 433,\n",
       " 14809,\n",
       " 10630,\n",
       " 179,\n",
       " 578,\n",
       " 485,\n",
       " 652,\n",
       " 180,\n",
       " 5811,\n",
       " 19883,\n",
       " 7039,\n",
       " 1374,\n",
       " 13839,\n",
       " 3144,\n",
       " 2159,\n",
       " 910,\n",
       " 439,\n",
       " 497,\n",
       " 435,\n",
       " 439,\n",
       " 540,\n",
       " 3311,\n",
       " 274,\n",
       " 435,\n",
       " 439,\n",
       " 250,\n",
       " 250,\n",
       " 1009,\n",
       " 12573,\n",
       " 13433,\n",
       " 450,\n",
       " 445,\n",
       " 1667,\n",
       " 1103,\n",
       " 247,\n",
       " 424,\n",
       " 2119,\n",
       " 434,\n",
       " 435,\n",
       " 144,\n",
       " 424,\n",
       " 135,\n",
       " 2400,\n",
       " 1641,\n",
       " 3345,\n",
       " 208,\n",
       " 2400,\n",
       " 1641,\n",
       " 1872,\n",
       " 2119,\n",
       " 439,\n",
       " 437,\n",
       " 433,\n",
       " 470,\n",
       " 433,\n",
       " 442,\n",
       " 433,\n",
       " 5088,\n",
       " 7103,\n",
       " 485,\n",
       " 253,\n",
       " 824,\n",
       " 253,\n",
       " 59,\n",
       " 2702,\n",
       " 11204,\n",
       " 17146,\n",
       " 1957,\n",
       " 1590,\n",
       " 17146,\n",
       " 305,\n",
       " 135,\n",
       " 3888,\n",
       " 6597,\n",
       " 3311,\n",
       " 12097,\n",
       " 4080,\n",
       " 557,\n",
       " 179,\n",
       " 92,\n",
       " 135,\n",
       " 448,\n",
       " 5777,\n",
       " 3480,\n",
       " 448,\n",
       " 5777,\n",
       " 17146,\n",
       " 1009,\n",
       " 422,\n",
       " 513,\n",
       " 13322,\n",
       " 401,\n",
       " 437,\n",
       " 3311,\n",
       " 478,\n",
       " 1612,\n",
       " 9251,\n",
       " 166,\n",
       " 337,\n",
       " 144,\n",
       " 4134,\n",
       " 24490,\n",
       " 17146,\n",
       " 144,\n",
       " 735,\n",
       " 424,\n",
       " 46,\n",
       " 448,\n",
       " 376,\n",
       " 47,\n",
       " 327,\n",
       " 6486,\n",
       " 415,\n",
       " 524,\n",
       " 435,\n",
       " 1641,\n",
       " 86,\n",
       " 16270,\n",
       " 8158,\n",
       " 8514,\n",
       " 1707,\n",
       " 437,\n",
       " 6423,\n",
       " 15779,\n",
       " 448,\n",
       " 52,\n",
       " 517,\n",
       " 9735,\n",
       " 18052,\n",
       " 3888,\n",
       " 3889,\n",
       " 16,\n",
       " 939,\n",
       " 3088,\n",
       " 57,\n",
       " 29564,\n",
       " 22274,\n",
       " 401,\n",
       " 305,\n",
       " 44312,\n",
       " 201,\n",
       " 171,\n",
       " 172,\n",
       " 8158,\n",
       " 8514,\n",
       " 435,\n",
       " 1102,\n",
       " 448,\n",
       " 6432,\n",
       " 448,\n",
       " 2813,\n",
       " 1856,\n",
       " 2041,\n",
       " 629,\n",
       " 3300,\n",
       " 484,\n",
       " 9904,\n",
       " 7,\n",
       " 154,\n",
       " 154,\n",
       " 3291,\n",
       " 36661,\n",
       " 111,\n",
       " 10528,\n",
       " 484,\n",
       " 2692,\n",
       " 86,\n",
       " 5288,\n",
       " 3778,\n",
       " 122,\n",
       " 3778,\n",
       " 17146,\n",
       " 12097,\n",
       " 2052,\n",
       " 2162,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_talk_word_index_exp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels_RNN = np.argmax(train_labels_onehot_shuffle, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 6, 0, 0, 3, 0, 4, 0, 0])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_RNN[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels_RNN = np.argmax(test_labels_onehot_shuffle, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, texts, lengths, labels):\n",
    "        self.texts = texts\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "        self.size = len(self.texts)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        temp = list(zip(self.texts, self.lengths, self.labels))\n",
    "        random.shuffle(temp)\n",
    "        self.texts, self.lengths, self.labels = zip(*temp)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res_texts = self.texts[self.cursor:self.cursor+n]\n",
    "        res_lengths = self.lengths[self.cursor:self.cursor+n]\n",
    "        res_labels = self.labels[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return res_texts, res_labels, res_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences\n",
      " ([2926, 2809, 1855, 6264, 111, 10735, 24652, 3683, 1462, 2677, 3383, 1825, 1850, 554, 148, 149, 148, 6252, 746, 1855, 22923, 11563, 24653, 309, 826, 8103, 2921, 3365, 3713, 6270, 826, 1153, 2921, 9744, 1009, 24654, 2424, 6270, 826, 24655, 2390, 3434, 1631, 24656, 3914, 10769, 1008, 4878, 1426, 852, 7847, 6270, 924, 3330, 1957, 9745, 6264, 111, 4052, 253, 824, 253, 3903, 2750, 490, 24657, 2368, 10735, 792, 6264, 24657, 10735, 2269, 24658, 1233, 3095, 24659, 16145, 2584, 127, 16145, 10337, 706, 8916, 170, 1345, 5811, 1981, 9567, 18489, 1, 1124, 13274, 2357, 263, 1957, 1012, 10735, 858, 199, 8818, 8817, 158, 13375, 24660, 199, 8817, 22923, 967, 378, 967, 578, 2110, 1957, 1038, 2110, 1668, 16516, 442, 2893, 3048, 7280, 826, 6443, 442, 24657, 356, 730, 9745, 1018, 2921, 3365, 6166, 17500, 5381, 10764, 24661, 3642, 1952, 2548, 3291, 967, 3914, 1419, 753, 318, 2506, 1227, 24662, 333, 2110, 24657, 3989, 24663, 6270, 1748, 6270, 12927, 967, 17500, 1957, 4157, 1345, 4157, 24664, 1260, 10094, 1767, 2170, 826, 1009, 16516, 1957, 15083, 5379, 5379, 1839, 876, 1240, 1303, 1303, 24665, 3042, 3873, 4802, 14429, 3319, 2727, 415, 8948, 24666, 10367, 4602, 1303, 40, 8948, 24666, 1703, 2110, 6801, 1024, 3836, 826, 192, 24667, 574, 3319, 2727, 415, 12941, 3989, 2240, 253, 19265, 3989, 13335, 6784, 138, 253, 138, 1578, 4033, 358, 4678, 177, 99, 3122, 3989, 138, 24668, 520, 629, 1031, 899, 8617, 3203, 24657, 3989, 1186, 149, 4, 6270, 1864, 6270, 1167, 2696, 24669, 22812, 47, 899, 4, 24657, 3989, 455, 6746, 108, 149, 2595, 1, 6086, 5097, 4878, 1, 1997, 6746, 108, 149, 1561, 4, 24657, 3989, 455, 5707, 6746, 108, 149, 1957, 20099, 899, 4, 24657, 8196, 1011, 99, 1, 4, 24657, 3989, 455, 5707, 217, 24670, 149, 1561, 149, 1561, 5097, 99, 1, 6876, 3574, 126, 578, 13501, 24670, 663, 4878, 1419, 13274, 826, 8527, 1549, 24670, 6086, 5097, 1, 5426, 24657, 10735, 826, 24670, 19382, 1595, 826, 24671, 3989, 2240, 253, 19265, 3989, 1083, 2510, 24672, 6166, 1748, 7453, 1748, 5707, 13312, 8345, 3836, 612, 1233, 3836, 4500, 3836, 1957, 13312, 24657, 3989, 7112, 149, 862, 454, 5987, 3836, 2276, 601, 1373, 1761, 967, 17500, 3781, 1240, 263, 23473, 2874, 1703, 3745, 812, 1460, 12941, 24657, 3989, 487, 1433, 2154, 7604, 1014, 277, 13692, 24657, 3989, 509, 4261, 24657, 3989, 1460, 653, 1463, 1962, 1329, 1464, 4261, 1587, 24663, 3989, 5707, 3554, 3783, 20, 138, 14052, 10185, 956, 372, 2660, 139, 2750, 490, 227, 10489, 227, 1164, 15785, 6206, 3783, 7034, 24657, 3989, 1, 3649, 1747, 4024, 77, 3685, 533, 2660, 139, 2750, 1720, 3989, 223, 1747, 4194, 3685, 732, 1747, 3584, 1971, 967, 51, 1005, 12941, 3708, 24663, 3989, 2750, 1091, 3934, 1018, 16085, 6801, 6270, 3934, 1018, 356, 356, 2704, 2963, 556, 356, 556, 556, 356, 2516, 106, 274, 274, 556, 2240, 356, 562, 563, 16085, 2540, 24673, 2696, 1915, 6264, 19521, 1018, 356, 714, 1973, 3363, 2548, 2696, 724, 3836, 616, 5426, 3934, 1018, 1043, 356, 24674, 2568, 7692, 24675, 3990, 24657, 3989, 508, 4790, 9794, 149, 3989, 6086, 6801, 585, 3542, 11082, 924, 1054, 3542, 2350, 1042, 1054, 3542, 4005, 7166, 4575, 287, 5178, 6047, 24657, 3989, 24676, 1864, 40, 585, 3836, 15984, 2968, 3081, 688, 6386, 2325, 17201, 6183, 3081, 1864, 11082, 2350, 1038, 3081, 688, 2325, 726, 6270, 2696, 2696, 241, 7112, 2696, 9043, 383, 16085, 1957, 1041, 2968, 2548, 2548, 1103, 5748, 2749, 2548, 3989, 263, 1957, 585, 2660, 156, 585, 906, 2660, 1145, 16085, 2660, 1433, 6886, 981, 6270, 3193, 2613, 6270, 3193, 1173, 263, 2660, 981, 2512, 981, 870, 24677, 4544, 6800, 6270, 924, 1054, 1480, 6270, 2705, 465, 2090, 826, 5707, 2777, 8707, 2630, 967, 2660, 694, 4059, 120, 6745, 3712, 3081, 487, 120, 3376, 47, 1723, 1039, 52, 1723, 1039, 2240, 6403, 508, 6022, 16085, 6097, 1038, 1957, 3836, 2660, 981, 2750, 816, 3836, 1723, 1039, 3669, 1038, 3081, 9077, 3953, 2750, 12941, 3989, 942, 521, 1720, 2660, 981, 1855, 13857, 3247, 10287, 3662, 12895, 5994, 3244, 1957, 3662, 12895, 5994, 774, 9374, 3569, 574, 3574, 3679, 1855, 3662, 12895, 5994, 1460, 24678, 3437, 10287, 4257, 1266, 2568, 961, 3081, 11292, 5951, 4184, 3643, 9374, 6229, 149, 562, 18834, 723, 9374, 2842, 832, 3452, 1814, 832, 3662, 24657, 3989, 3660, 14643, 24679, 33, 234, 12300, 616, 17757, 3662, 4184, 3662, 12895, 5994, 3568, 4579, 4184, 6762, 2075, 1972, 612, 6639, 3781, 774, 3892, 463, 383, 183, 3452, 4184, 4184, 2660, 692, 64, 149, 24680, 13854, 11650, 35, 24675, 3990, 4202, 1962, 3892, 24657, 3989, 967, 24657, 3989, 2710, 526, 3892, 24675, 3990, 393, 3543, 6126, 9398, 3062, 24681, 6035, 345, 311, 24657, 3989, 1186, 149, 3915, 2761, 1531, 1496, 4375, 278, 2192, 19265, 833, 4553, 393, 2528, 24657, 3989, 19992, 4790, 149, 425, 446, 86, 3244, 24675, 3989, 6411, 24675, 3989, 826, 465, 24675, 3989, 1007, 1445, 3989, 11884, 149, 24675, 3990, 11884, 3455, 6086, 1038, 5961, 1017, 148, 5961, 1017, 10379, 533, 5994, 6252, 10379, 148, 94, 10379, 94, 6801, 594, 585, 94, 1043, 148, 94, 1103, 64, 24657, 3989, 2660, 967, 4005, 16085, 874, 594, 585, 1038, 3836, 471, 1041, 2968, 2548, 2548, 5748, 2749, 2548, 24657, 3989, 3836, 17043, 471, 1666, 64, 38, 508, 16085, 967, 1217, 967, 3781, 205, 97, 24657, 2368, 967, 4224, 13282, 2357, 967, 6171, 1281, 24657, 3781, 21423, 1303, 1772, 876, 205, 97, 207, 64, 2639, 772, 20126, 4224, 13282, 1915, 24, 24657, 13032, 14211, 533, 3989, 1479, 392, 149, 148, 10379, 1038, 2895, 3713, 585, 40, 585, 3836, 967, 526, 24657, 2368, 5426, 3989, 24657, 3989, 1303, 1479, 6086, 1971, 205, 793, 126, 1477, 24657, 3989, 1043, 1186, 149, 1186, 149, 1463, 6270, 277, 2540, 6270, 5374, 6139, 13327, 5616, 7578, 5707, 3990, 24682, 6935, 533, 5374, 1186, 94, 1038, 918, 1479, 533, 5374, 2704, 2963, 556, 2832, 898, 9616, 2325, 898, 8996, 3203, 3393, 64, 24657, 3989, 7020, 1091, 3463, 5374, 1957, 1219, 4184, 227, 907, 13048, 6784, 2551, 761, 13048, 9275, 10967, 774, 460, 1295, 24683, 24684, 24685, 24686, 427, 24687, 508, 2671, 1038, 24686, 601, 3836, 8345, 2119, 4011, 9842, 2325, 726, 2548, 1467, 17500, 1303, 24665, 3042, 3873, 4802, 14429, 3319, 494, 1056, 3873, 957, 6801, 6358, 14211, 1217, 1, 5012, 6358, 1490, 24688, 2120, 6280, 4224, 2379, 24670, 6086, 918, 726, 24689, 1103, 5623, 918, 2120, 2119, 3789, 2823, 24690, 24689, 24670, 918, 24691, 2120, 5829, 285, 4184, 65, 3789, 13305, 24692, 24689, 425, 24670, 192, 2120, 5829, 17044, 229, 8356, 1957, 726, 2548, 3244, 6331, 616, 17515, 24693, 24689, 425, 24670, 192, 1533, 6923, 876, 3692, 3244, 278, 1008, 458, 17500, 6801, 24670, 6358, 1490, 3805, 2120, 6280, 51, 24694, 508, 24695, 4087, 24695, 427, 907, 2789, 2789, 12300, 6330, 3712, 704, 1479, 918, 1957, 3822, 3095, 24696, 6262, 3989, 612, 24671, 3322, 3114, 24697, 3990, 9208, 16145, 2742, 3480, 21260, 663, 3272, 3114, 24657, 10735, 4308, 4827, 596, 24657, 10735, 1997, 99, 204, 4878, 204, 7109, 10859, 8196, 4554, 1683, 4823, 17932, 5398, 177, 12941, 3989, 1820, 2742, 626, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2813, 2823, 2410, 27126, 2410, 6, 6176, 2135, 216, 2006, 1012, 833, 7959, 1919, 1454, 39, 444, 1991, 9073, 489, 653, 14608, 399, 1459, 5443, 39, 10465, 7958, 92, 34816, 1723, 715, 1265, 32123, 2617, 16491, 4669, 526, 186, 2657, 784, 200, 6716, 3397, 1723, 1952, 10529, 724, 3339, 7719, 14364, 27122, 1919, 790, 1580, 2045, 5082, 2899, 673, 2113, 2057, 6741, 6705, 748, 452, 9, 1919, 14678, 34817, 34818, 14095, 11691, 1095, 569, 1533, 3647, 29896, 989, 10465, 1034, 924, 807, 810, 78, 1572, 640, 6279, 12198, 2292, 34819, 9771, 1267, 11657, 726, 6148, 2947, 2862, 1009, 16640, 34820, 3030, 730, 11803, 307, 1633, 1459, 1459, 2667, 1459, 9975, 34821, 325, 5636, 2778, 1595, 1587, 7958, 1533, 2924, 2816, 34822, 7019, 653, 16344, 1804, 1804, 2924, 12486, 2924, 3581, 5619, 6171, 266, 2410, 907, 7817, 4869, 5026, 4869, 737, 135, 4248, 2947, 62, 1454, 10925, 5354, 5357, 1176, 1355, 307, 3647, 1855, 11065, 1544, 755, 755, 4374, 737, 135, 1500, 5385, 17653, 1500, 10781, 509, 34823, 3014, 356, 1367, 1737, 2880, 13071, 2225, 16046, 6, 1549, 1416, 4674, 25, 2884, 3285, 3812, 2506, 3675, 3675, 4918, 206, 4834, 9514, 28890, 1667, 5286, 1142, 1511, 34824, 1142, 19311, 1142, 429, 10867, 5951, 51, 1804, 764, 1804, 34825, 1312, 5227, 2006, 1499, 15896, 12486, 30081, 2924, 2924, 1095, 7302, 553, 126, 1237, 154, 3685, 2767, 3685, 20088, 3397, 1451, 1459, 34816, 1581, 308, 399, 34816, 2924, 383, 6, 6144, 912, 5669, 752, 6880, 34826, 28151, 27126, 9935, 5801, 590, 1459, 807, 1007, 1268, 6927, 34816, 2802, 533, 1416, 4248, 2874, 493, 378, 1460, 809, 34646, 809, 34002, 4, 7228, 16411, 4, 5015, 1984, 34816, 10082, 6172, 1544, 20676, 784, 4989, 12234, 1591, 15673, 774, 919, 12308, 442, 34827, 643, 14095, 11691, 6586, 6, 452, 452, 452, 1501, 2139, 643, 1533, 167, 919, 920, 31756, 9072, 5173, 8, 1962, 119, 119, 117, 117, 11105, 7719, 117, 11105, 6686, 117, 11105, 34828, 11105, 6686, 8150, 1619, 119, 132, 1619, 1619, 493, 562, 828, 34816, 1459, 828, 819, 26940, 2256, 480, 2183, 9855, 31756, 4099, 14279, 462, 819, 1567, 547, 1265, 274, 444, 2680, 5949, 1855, 16012, 33069, 33069, 33069, 33069, 33069, 1105, 2750, 33069, 33069, 33069, 33069, 33069, 4706, 5618, 451, 444, 5806, 3722, 8784, 1095, 399, 9576, 465, 11073, 34829, 1855, 16012, 383, 451, 1229, 34830, 1855, 11796, 434, 784, 472, 577, 824, 20751, 31516, 565, 8502, 1632, 7719, 1371, 3033, 2192, 371, 5271, 367, 34831, 880, 1267, 14706, 16524, 1833, 307, 9072, 2980, 51, 4222, 5402, 533, 11346, 3284, 563, 828, 2555, 1990, 2555, 34816, 1459, 562, 3907, 11265, 480, 480, 326, 12009, 12010, 21, 78, 2549, 652, 12009, 12010, 7014, 1748, 2118, 2515, 21363, 30256, 213, 1281, 205, 205, 46, 629, 126, 126, 46, 3698, 14678, 3048, 5417, 34832, 34832, 15879, 34816, 2318, 573, 52, 1368, 5929, 6994, 4040, 1580, 828, 7025, 750, 1308, 2129, 119, 573, 750, 11691, 2012, 1308, 2337, 2135, 1368, 1695, 117, 2568, 100, 1536, 784, 9073, 1536, 537, 1416, 27562, 399, 2617, 809, 2318, 4452, 9842, 5949, 1475, 12315, 4394, 525, 3806, 78, 4248, 737, 135, 1500, 962, 4063, 34833, 3933, 11831, 7817, 624, 2337, 2632, 1761, 10168, 3789, 64, 10168, 2135, 1163, 2135, 9932, 465, 3680, 2118, 2957, 4408, 2726, 3417, 2726, 10168, 21069, 2410, 573, 5914, 12994, 2924, 1014, 12907, 3275, 2660, 626, 1173, 20676, 784, 2719, 12994, 2924, 1919, 126, 126, 1312, 12994, 4882, 745, 717, 34834, 745, 717, 444, 34835, 10781, 31286, 717, 34816, 1400, 34836, 45, 1282, 1459, 828, 2053, 2047, 6586, 1674, 2697, 784, 22615, 33, 34837, 912, 1225, 918, 34838, 34816, 1213, 2709, 6586, 1142, 2410, 4984, 19487, 3581, 34839, 10252, 1740, 1416, 27562, 3691, 1500, 561, 912, 1269, 1459, 4425, 509, 640, 34840, 1460, 167, 2632, 34841, 2823, 1460, 4439, 730, 94, 47, 20443, 5169, 586, 832, 425, 26670, 39, 586, 3897, 1460, 399, 5067, 42, 480, 708, 879, 52, 717, 3244, 5847, 7918, 910, 910, 910, 1459, 8922, 4024, 34816, 34816, 13553, 34816, 34069, 22664, 6591, 8052, 21485, 2884, 86, 2042, 5169, 138, 13898, 879, 2823, 588, 653, 5271, 3401, 2212, 19152, 2822, 3336, 11735, 3449, 5209, 2742, 493, 34842, 1031, 1957, 8098, 4772, 3649, 1587, 1027, 4204, 34843, 5443, 3581, 5879, 6940, 480, 5227, 23112, 442, 627, 200, 399, 3109, 1156, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2430, 2639, 1900, 799, 13199, 6176, 8895, 6688, 4728, 1014, 956, 3573, 15523, 48692, 33063, 992, 3244, 3449, 2689, 31154, 799, 695, 119, 3193, 3473, 20109, 695, 2174, 8386, 2689, 31154, 5138, 3193, 2862, 255, 988, 16278, 28767, 825, 48693, 2862, 1952, 11236, 48694, 227, 3973, 826, 557, 93, 436, 442, 43004, 42, 25198, 2814, 444, 5908, 2514, 2050, 1281, 3879, 2905, 1183, 4678, 214, 11028, 26522, 5424, 13322, 629, 2817, 214, 1947, 617, 415, 2183, 3735, 4676, 1013, 2880, 28926, 11624, 858, 15813, 6467, 2192, 956, 4066, 4159, 3338, 690, 3919, 3373, 629, 675, 16109, 466, 78, 3108, 2890, 14037, 1536, 48695, 3404, 2762, 1014, 7993, 2778, 2956, 799, 577, 493, 11121, 1150, 799, 4999, 1273, 4892, 560, 2750, 4871, 907, 27748, 907, 5656, 4066, 1034, 876, 1250, 99, 8703, 5724, 9294, 5719, 3384, 32603, 3219, 1900, 3973, 333, 205, 2003, 2516, 879, 2549, 876, 5540, 5540, 1747, 1219, 1828, 1611, 2896, 2039, 14461, 12677, 2956, 12677, 5685, 1265, 1903, 2594, 13199, 8665, 99, 6010, 21, 19274, 810, 1088, 132, 51, 2083, 1722, 1540, 10666, 504, 276, 1674, 4731, 444, 17008, 1714, 209, 22671, 6857, 1339, 669, 4731, 4735, 1303, 832, 1480, 75, 3015, 372, 561, 907, 7817, 4617, 415, 43, 2236, 18056, 405, 8895, 3320, 214, 11028, 551, 28993, 8368, 307, 568, 2476, 833, 17646, 8196, 778, 695, 1947, 18630, 1010, 4527, 16950, 28682, 20, 561, 13322, 561, 7645, 2638, 1178, 2817, 37371, 4827, 2890, 428, 775, 21, 4162, 3187, 307, 51, 5083, 24862, 21626, 17294, 2874, 6585, 1317, 47, 5111, 8862, 2459, 1002, 1233, 3167, 32146, 15308, 6624, 2880, 9811, 9976, 634, 10400, 11624, 36848, 11624, 21041, 11624, 337, 401, 4430, 4442, 2109, 832, 38445, 156, 48696, 8895, 17886, 575, 9015, 15029, 3985, 699, 2059, 19939, 48321, 6267, 1038, 48697, 48698, 1014, 6267, 11624, 6267, 24431, 11624, 6267, 13647, 10527, 9355, 425, 322, 24862, 7516, 9251, 2141, 1525, 91, 2569, 451, 452, 14095, 1906, 1774, 99, 1426, 2564, 2141, 7018, 2569, 1648, 452, 14095, 1906, 834, 5540, 4655, 6267, 1839, 8251, 3600, 1748, 1831, 1303, 9693, 4271, 1831, 1103, 6267, 14117, 14095, 1906, 1774, 1426, 2564, 2141, 7018, 14117, 2360, 39782, 48699, 11745, 3189, 832, 12927, 15731, 21, 214, 242, 856, 1648, 670, 1855, 1088, 156, 8895, 13366, 4999, 413, 48700, 24862, 133, 192, 1501, 5213, 35903, 1544, 48701, 490, 2283, 250, 6010, 21, 167, 12271, 20594, 1182, 1200, 307, 5707, 9249, 1860, 616, 762, 27714, 5284, 1034, 423, 275, 3911, 586, 422, 1501, 5418, 4375, 465, 4080, 2641, 4133, 1947, 962, 4708, 4004, 29801, 688, 32147, 16980, 13956, 20619, 48702, 8638, 967, 1595, 7959, 6346, 21893, 2109, 6161, 2575, 48701, 16047, 1174, 383, 1176, 11624, 48703, 250, 4878, 1774, 1544, 509, 21556, 6358, 799, 48704, 48705, 34770, 32147, 737, 513, 1930, 3108, 23767, 48703, 1308, 1157, 1308, 864, 6940, 1071, 706, 2269, 52, 1839, 21, 1544, 158, 48703, 2516, 879, 2087, 1546, 48703, 1546, 1219, 15680, 575, 48706, 3756, 11031, 4878, 6087, 1106, 1237, 48707, 5658, 6425, 48708, 6425, 48708, 3384, 29953, 1719, 2993, 30504, 1460, 12, 17127, 2158, 3019, 48709, 39739, 48710, 32147, 48711, 627, 4143, 99, 18966, 24862, 229, 229, 10168, 1544, 9485, 27184, 11760, 48703, 39158, 33108, 1345, 6155, 39158, 359, 2087, 1546, 5296, 383, 93, 18664, 11803, 307, 565, 748, 452, 4878, 1873, 292, 4479, 4187, 1288, 1546, 17137, 307, 31348, 307, 267, 7602, 399, 135, 36848, 42713, 35969, 48703, 52, 307, 1303, 1163, 9485, 7696, 9485, 6010, 33063, 267, 5987, 48703, 10373, 1616, 8023, 17404, 4181, 2428, 214, 1150, 5374, 1142, 3291, 1667, 505, 561, 553, 3323, 5424, 105, 746, 6741, 183, 17997, 48703, 66, 8368, 307, 1421, 170, 48712, 1936, 5167, 3413, 2960, 16598, 34127, 2484, 170, 2449, 1868, 591, 307, 1237, 491, 2158, 3019, 500, 6100, 66, 502, 170, 20634, 2134, 2158, 48713, 3413, 66, 12008, 21658, 774, 3006, 11017, 15680, 5017, 510, 1071, 490, 2036, 3809, 2806, 624, 2142, 4005, 3192, 197, 6425, 48708, 3770, 571, 48714, 6044, 12765, 4345, 6169, 48714, 24608, 22382, 6267, 1303, 13647, 1648, 1648, 109, 48703, 186, 4076, 8022, 1649, 2862, 980, 299, 1663, 24608, 1839, 12502, 31060, 3600, 6983, 48715, 48716, 48703, 48717, 6425, 48708, 21041, 21626, 746, 1955, 24608, 2051, 247, 26585, 12088, 197, 4254, 3142, 2428, 629, 27090, 24608, 6267, 197, 4254, 3142, 1900, 2428, 5116, 2885, 27090, 6267, 35481, 3390, 6470, 4254, 2727, 2178, 1892, 48708, 12806, 6267, 2846, 1989, 4793, 491, 1894, 1939, 1503, 16008, 209, 307, 545, 588, 4096, 1831, 602, 1939, 6425, 48708, 545, 1652, 307, 35903, 21639, 755, 2398, 537, 3120, 5352, 627, 23545, 2050, 3142, 229, 1535, 872, 9529, 6166, 46632, 2184, 1939, 22382, 573, 48718, 5559, 2262, 1142, 8930, 132, 48719, 86, 3235, 8930, 912, 588, 4096, 1598, 45791, 10021, 18033, 2665, 1023, 9529, 6166, 4230, 2516, 9669, 4, 4, 4365, 24, 739, 132, 6267, 254, 596, 6774, 10730, 690, 11824, 1308, 6169, 227, 15680, 1900, 1067, 48695, 6193, 48703, 809, 466, 48720, 1014, 48720, 1690, 6441, 48720, 2712, 509, 33222, 2428, 48720, 8040, 5216, 6686, 15338, 4361, 17125, 3181, 93, 6169, 14232, 10529, 48703, 4078, 48703, 307, 473, 2564, 536, 2174, 488, 22515, 7715, 48721, 7715, 307, 473, 695, 2902, 29192, 1804, 4406, 6193, 5116, 2428, 138, 14956, 6169, 14232, 8864, 5707, 19434, 1117, 32904, 5418, 16039, 6805, 518, 2052, 2318, 1605, 2636, 490, 10057, 4389, 10057, 13191, 3031, 48703, 48720, 2719, 46, 10166, 2109, 3815, 548, 6169, 48703, 333, 690, 1014, 832, 48722, 48723, 7578, 4249, 48703, 1648, 8022, 1649, 1544, 9485, 688, 3272, 3114, 7194, 8022, 1649, 1867, 38, 12207, 5801, 6857, 7448, 31690, 1772, 539, 14268, 539, 14268, 3685, 2641, 1004, 505, 5601, 7578, 967, 7578, 2486, 10373, 1616, 3703, 1319, 3084, 307, 48703, 1121, 48703, 15680, 967, 7578, 8022, 2564, 307, 2220, 17481, 25457, 1990, 255, 2862, 626, 16779, 729, 726, 2013, 544, 2013, 91, 7578, 634, 4775, 48703, 2525, 6169, 4341, 114, 539, 14268, 1504, 3272, 3114, 9890, 10493, 456, 456, 748, 456, 208, 343, 21041, 5296, 48703, 655, 4309, 307, 340, 99, 19766, 5005, 539, 10168, 7710, 267, 561, 48723, 561, 4254, 3244, 179, 3635, 8022, 1649, 1544, 9485, 20592, 2273, 9151, 742, 48703, 2133, 2133, 5416, 4999, 1968, 4999, 399, 36848, 42713, 35969, 2921, 1598, 2283, 99, 1544, 415, 23668, 2385, 4181, 4034, 30479, 4157, 12727, 1158, 5834, 2071, 48724, 7959, 1023, 1002, 7997, 8368, 307, 4422, 1318, 2816, 229, 575, 555, 925, 12, 31727, 4624, 235, 8040, 9485, 5361, 779, 6936, 1503, 4644, 4476, 1072, 690, 1105, 1645, 1530, 276, 1561, 3684, 2084, 1166, 21, 3329, 7959, 227, 3929, 907, 31, 907, 3694, 719, 307, 11624, 8665, 8753, 8895, 18, 48725, 1562, 7817, 799, 4375, 3320, 2122, 2123, 25359, 27099, 10871, 214, 1947, 30124, 4678, 35802, 16523, 48703, 12927, 356, 2498, 4, 557, 4345, 42922, 18, 48726, 1837, 11707, 1009, 229, 575, 1117, 568, 36681, 31869, 11028, 885, 27090, 2070, 2099, 135, 1424, 620, 27090, 14436, 1930, 2070, 9002, 6010, 21, 13531, 138, 4775, 27090, 14436, 685, 4168, 1943, 9935, 48703, 1991, 2512, 1859, 703, 1533, 1014, 2884, 277, 34613, 477, 48703, 2512, 1077, 2890, 2041, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Target values\n",
      " (0, 0, 0)\n",
      "\n",
      "Sequence lengths\n",
      " (1163, 706, 1162)\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(train_talk_word_index_exp, train_lenghts, train_labels_RNN)\n",
    "d = data.next_batch(3)\n",
    "print('Input sequences\\n', d[0], end='\\n\\n')\n",
    "print('Target values\\n', d[1], end='\\n\\n')\n",
    "print('Sequence lengths\\n', d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835, 2221)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 2221)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_labels_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_labels_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51962"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = len(vocab)+1,\n",
    "    state_size = 100,\n",
    "    batch_size = 50,\n",
    "    num_classes = 8):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(1.0)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, 2*state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                  initial_state=init_state, dtype=tf.float32)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    #rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    \n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "    #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    \n",
    "    last_rnn_output = tf.reduce_mean(rnn_outputs, 1)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def train_graph(graph, batch_size = 50, num_epochs = 10, iterator = SimpleDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        tr = iterator(train_talk_word_index_exp, train_lenghts, train_labels_RNN)\n",
    "        te = iterator(test_talk_word_index_exp, test_lenghts, test_labels_RNN)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fc3cc8e3eb8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "g = build_graph(vocab_size=len(vocab)+1, batch_size=50, num_classes=8, state_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.517837840762 - te: 0.593333333731\n",
      "Accuracy after epoch 2  - tr: 0.532222221295 - te: 0.576000010967\n",
      "Accuracy after epoch 3  - tr: 0.537222221494 - te: 0.588000011444\n",
      "Accuracy after epoch 4  - tr: 0.535555551449 - te: 0.571999996901\n",
      "Accuracy after epoch 5  - tr: 0.534444442226 - te: 0.596000003815\n",
      "Accuracy after epoch 6  - tr: 0.536111108959 - te: 0.596000003815\n",
      "Accuracy after epoch 7  - tr: 0.531111109588 - te: 0.603999996185\n",
      "Accuracy after epoch 8  - tr: 0.532222222951 - te: 0.579999995232\n",
      "Accuracy after epoch 9  - tr: 0.539444446564 - te: 0.576000005007\n",
      "Accuracy after epoch 10  - tr: 0.533333338797 - te: 0.583999991417\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses = train_graph(g, batch_size = 50, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = len(vocab)+1,\n",
    "    state_size = 100,\n",
    "    batch_size = 50,\n",
    "    num_classes = 8):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(1.0)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    fw_cell = tf.contrib.rnn.LSTMCell(state_size)\n",
    "    bw_cell = tf.contrib.rnn.LSTMCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, 2*state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                      dtype=tf.float32)\n",
    "    \n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    #rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    \n",
    "    print(np.shape(rnn_outputs[0]))\n",
    "    \n",
    "    last_rnn_output = tf.concat([rnn_outputs[0], rnn_outputs[1]], 2)\n",
    "    \n",
    "    \n",
    "    last_rnn_output = tf.reduce_mean(last_rnn_output, 1)\n",
    "    print(np.shape(last_rnn_output))\n",
    "\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [2*state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "def train_graph(graph, batch_size = 50, num_epochs = 10, iterator = SimpleDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        tr = iterator(train_talk_word_index_exp, train_lenghts, train_labels_RNN)\n",
    "        te = iterator(test_talk_word_index_exp, test_lenghts, test_labels_RNN)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, ?, 100)\n",
      "(50, 200)\n"
     ]
    }
   ],
   "source": [
    "g = build_graph(vocab_size=len(vocab)+1, batch_size=50, num_classes=8, state_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-56-26c1824813c0>:73: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Accuracy after epoch 1  - tr: 0.531351346019 - te: 0.543333331744\n",
      "Accuracy after epoch 2  - tr: 0.536111111442 - te: 0.520000010729\n",
      "Accuracy after epoch 3  - tr: 0.543888888425 - te: 0.539999997616\n",
      "Accuracy after epoch 4  - tr: 0.536111106475 - te: 0.580000007153\n",
      "Accuracy after epoch 5  - tr: 0.54666666521 - te: 0.512000012398\n",
      "Accuracy after epoch 6  - tr: 0.538888887399 - te: 0.524000000954\n",
      "Accuracy after epoch 7  - tr: 0.543888889253 - te: 0.551999992132\n",
      "Accuracy after epoch 8  - tr: 0.540555556615 - te: 0.55600001812\n",
      "Accuracy after epoch 9  - tr: 0.543333335055 - te: 0.532000005245\n",
      "Accuracy after epoch 10  - tr: 0.542777774235 - te: 0.543999999762\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses = train_graph(g, batch_size = 50, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = len(vocab)+1,\n",
    "    state_size = 100,\n",
    "    batch_size = 50,\n",
    "    hidden_states = 30,\n",
    "    num_classes = 8):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(1.0)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    fw_cell = tf.contrib.rnn.LSTMCell(state_size)\n",
    "    bw_cell = tf.contrib.rnn.LSTMCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, 2*state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                      dtype=tf.float32)\n",
    "    \n",
    "    #print(np.shape(rnn_outputs))\n",
    "    \n",
    "    #rnn_outputs = tf.concat([rnn_outputs[0], rnn_outputs[1]], 2)\n",
    "    \n",
    "    #print(np.shape(rnn_outputs))\n",
    "    \n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    #rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    \n",
    "    #print(np.shape(rnn_outputs))\n",
    "\n",
    "    #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, 2*state_size]), idx)\n",
    "    \n",
    "    #print(np.shape(last_rnn_output))\n",
    "    \n",
    "    #print(np.shape(rnn_outputs[0]))\n",
    "    \n",
    "    last_rnn_output = tf.concat([rnn_outputs[0], rnn_outputs[1]], 2)\n",
    "    \n",
    "    \n",
    "    last_rnn_output = tf.reduce_mean(last_rnn_output, 1)\n",
    "    #print(np.shape(last_rnn_output))\n",
    "\n",
    "    # Hidden layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [2*state_size, hidden_states])\n",
    "        b = tf.get_variable('b', [hidden_states], initializer=tf.constant_initializer(0.0))\n",
    "    h = tf.matmul(last_rnn_output, W) + b\n",
    "    \n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        V = tf.get_variable('V', [hidden_states, num_classes])\n",
    "        c = tf.get_variable('c', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(h, V) + c\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "def train_graph(graph, batch_size = 50, num_epochs = 10, iterator = SimpleDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        tr = iterator(train_talk_word_index_exp, train_lenghts, train_labels_RNN)\n",
    "        te = iterator(test_talk_word_index_exp, test_lenghts, test_labels_RNN)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = build_graph(vocab_size=len(vocab)+1, batch_size=50, num_classes=8, state_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-167-29fb8aa73f67>:89: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Accuracy after epoch 1  - tr: 0.525945946373 - te: 0.57333334287\n",
      "Accuracy after epoch 2  - tr: 0.540000002417 - te: 0.540000003576\n",
      "Accuracy after epoch 3  - tr: 0.531666667925 - te: 0.596000003815\n",
      "Accuracy after epoch 4  - tr: 0.536111109787 - te: 0.548000007868\n",
      "Accuracy after epoch 5  - tr: 0.547222222719 - te: 0.543999993801\n",
      "Accuracy after epoch 6  - tr: 0.533333336314 - te: 0.587999999523\n",
      "Accuracy after epoch 7  - tr: 0.539444444908 - te: 0.572000002861\n",
      "Accuracy after epoch 8  - tr: 0.536111110614 - te: 0.539999997616\n",
      "Accuracy after epoch 9  - tr: 0.537222225633 - te: 0.580000001192\n",
      "Accuracy after epoch 10  - tr: 0.565555558436 - te: 0.544000005722\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses = train_graph(g, batch_size = 50, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs have an advantage because for the words are inputted in a sequence therefore there is a possiblity that the NN can actually infer something from the sequence rather than just the bag of words representation which completelly ignores order and context. However, this method takes significantly longer and is even more suceptable to data availablility, because contex is now something else that needs to learned and it requires extra data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the final state as the word representation actually started off better than bag of words but ended up just predicting ooo. Dropout seems to be the biggest help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different architectures (LSTM, GRU, Standard RNN cell) no significant differences were found between the model accuracies. We blame it on the data once more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biderectional LSTM also didnt seem to help much and in fact did worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding an additional hidden layer the result stayed the same."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
