{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NLP Practical 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "    \n",
    "raw_text = doc.xpath('//content/text()')\n",
    "raw_label = doc.xpath('//head/keywords/text()')\n",
    "\n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_sentences = []\n",
    "talknum = len(raw_text)\n",
    "\n",
    "for i in range(talknum):\n",
    "    temp = re.sub(r'\\([^)]*\\)', '', raw_text[i])\n",
    "    temp = re.sub(r'\\n', '', raw_text[i])\n",
    "    temp = temp.split('.')\n",
    "    talk_sentences.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(talk_sentences[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "talk_sentence_word = []\n",
    "\n",
    "for talk in talk_sentences:\n",
    "    temp = []\n",
    "    for sent in talk:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent.lower()).split()\n",
    "        temp.append(tokens)#\n",
    "    talk_sentence_word.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_sentence_word[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "\n",
    "for talk in talk_sentence_word:\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            if word in freq:\n",
    "                freq[word] += 1\n",
    "            else:\n",
    "                freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts_ted_top1000 = []\n",
    "words_top_ted = []\n",
    "\n",
    "for word, count in Counter(freq).most_common(1000):\n",
    "    counts_ted_top1000.append(count)\n",
    "    words_top_ted.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_sentence_word_nostop = talk_sentence_word\n",
    "\n",
    "for stop in words_top_ted[:200]:\n",
    "    for talk in talk_sentence_word_nostop:\n",
    "        for sent in talk:\n",
    "            for word in sent:\n",
    "                if word == stop:\n",
    "                    sent.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_sentence_word_nostop[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(talk_sentence_word_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_label = []\n",
    "\n",
    "for i in range(len(raw_label)):\n",
    "    temp = re.sub(r' ', '', raw_label[i])\n",
    "    input_label.append(temp.split(','))\n",
    "    \n",
    "#print(input_label_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_binary = []\n",
    "\n",
    "for i in range(len(input_label)):\n",
    "    temp = np.zeros(3)\n",
    "    if 'technology' in '~'.join(input_label[i]):\n",
    "        temp[0] = 1\n",
    "    if 'entertainment' in '~'.join(input_label[i]):\n",
    "        temp[1] = 1\n",
    "    if 'design' in '~'.join(input_label[i]):\n",
    "        temp[2] = 1\n",
    "    labels_binary.append(temp)\n",
    "        \n",
    "print(input_label[9])\n",
    "print(labels_binary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = list(zip(talk_sentence_word_nostop, labels_binary))\n",
    "random.shuffle(temp)\n",
    "talk_sentence_word_nostop_shuffle, labels_binary_shuffle = zip(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "labels_onehot = []\n",
    "for i in range(len(input_label)):\n",
    "    temp = np.zeros(8)\n",
    "    temp[labels_binary[i][0]*4+labels_binary[i][1]*2+labels_binary[i][2]*1] = 1\n",
    "    labels_onehot.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_onehot_shuffle = []\n",
    "for i in range(len(input_label)):\n",
    "    temp = np.zeros(8)\n",
    "    temp[labels_binary_shuffle[i][0]*4+labels_binary_shuffle[i][1]*2+labels_binary_shuffle[i][2]*1] = 1\n",
    "    labels_onehot_shuffle.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = talk_sentence_word_nostop[:1835]\n",
    "test_data = talk_sentence_word_nostop[1835:]\n",
    "\n",
    "train_labels = labels_binary[:1835]\n",
    "test_labels = labels_binary[1835:]\n",
    "\n",
    "train_labels_onehot = labels_onehot[:1835]\n",
    "test_labels_onehot = labels_onehot[1835:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_shuffle = talk_sentence_word_nostop_shuffle[:1835]\n",
    "test_data_shuffle = talk_sentence_word_nostop_shuffle[1835:]\n",
    "\n",
    "train_labels_shuffle = labels_binary_shuffle[:1835]\n",
    "test_labels_shuffle = labels_binary_shuffle[1835:]\n",
    "\n",
    "train_labels_onehot_shuffle = labels_onehot_shuffle[:1835]\n",
    "test_labels_onehot_shuffle = labels_onehot_shuffle[1835:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(talk_sentence_word_nostop))\n",
    "print(np.shape(labels_binary))\n",
    "print(np.shape(labels_onehot))\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(train_labels_onehot))\n",
    "print(np.shape(test_data))\n",
    "print(np.shape(test_labels))\n",
    "print(np.shape(test_labels_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(talk_sentence_word_nostop_shuffle))\n",
    "print(np.shape(labels_binary_shuffle))\n",
    "print(np.shape(labels_onehot_shuffle))\n",
    "print(np.shape(train_data_shuffle))\n",
    "print(np.shape(train_labels_shuffle))\n",
    "print(np.shape(train_labels_onehot_shuffle))\n",
    "print(np.shape(test_data_shuffle))\n",
    "print(np.shape(test_labels_shuffle))\n",
    "print(np.shape(test_labels_onehot_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check = np.asarray(train_labels_onehot)\n",
    "print(check.sum(0) / check.sum())\n",
    "check = np.asarray(test_labels_onehot)\n",
    "print(check.sum(0) / check.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check = np.asarray(train_labels_onehot_shuffle)\n",
    "print(check.sum(0) / check.sum())\n",
    "check = np.asarray(test_labels_onehot_shuffle)\n",
    "print(check.sum(0) / check.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lenghts = []\n",
    "\n",
    "for talk in train_data:\n",
    "    N = 0\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            N += 1\n",
    "    train_lenghts.append(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lenghts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lenghts = []\n",
    "\n",
    "for talk in test_data:\n",
    "    N = 0\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            N += 1\n",
    "    test_lenghts.append(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_lenghts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_talk_word = []\n",
    "\n",
    "for talk in train_data:\n",
    "    temp = []\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            temp.append(word)\n",
    "    train_talk_word.append(temp)\n",
    "    \n",
    "print(len(train_talk_word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_talk_word = []\n",
    "\n",
    "for talk in test_data:\n",
    "    temp = []\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            temp.append(word)\n",
    "    test_talk_word.append(temp)\n",
    "    \n",
    "print(len(test_talk_word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_talk_word[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "N = 1\n",
    "\n",
    "for talk in train_data:\n",
    "    for sent in talk:\n",
    "        for word in sent:\n",
    "            if word in vocab:\n",
    "                N = N\n",
    "            else:\n",
    "                vocab[word] = N\n",
    "                N += 1\n",
    "                \n",
    "vocab[\"unknown_word\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"unknown_word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_talk_word_index = []\n",
    "\n",
    "for talk in train_talk_word:\n",
    "    temp = []\n",
    "    for word in talk:\n",
    "        if word in vocab:\n",
    "            temp.append(vocab[word])\n",
    "        else:\n",
    "            temp.append(vocab[\"unknown_word\"])\n",
    "    train_talk_word_index.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_talk_word_index = []\n",
    "\n",
    "for talk in test_talk_word:\n",
    "    temp = []\n",
    "    for word in talk:\n",
    "        if word in vocab:\n",
    "            temp.append(vocab[word])\n",
    "        else:\n",
    "            temp.append(vocab[\"unknown_word\"])\n",
    "    test_talk_word_index.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_talk_word_index[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_talk_word_index_exp = []\n",
    "\n",
    "for i in range(len(train_talk_word_index)):\n",
    "    temp = []\n",
    "    for j in range(max(train_lenghts)):\n",
    "        if j <= (train_lenghts[i]-1):\n",
    "            temp.append(train_talk_word_index[i][j])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    train_talk_word_index_exp.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_talk_word_index_exp = []\n",
    "\n",
    "for i in range(len(test_talk_word_index)):\n",
    "    temp = []\n",
    "    for j in range(max(train_lenghts)):\n",
    "        if j <= (test_lenghts[i]-1):\n",
    "            temp.append(test_talk_word_index[i][j])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    test_talk_word_index_exp.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(test_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(train_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_talk_word_index_exp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels_RNN = np.argmax(train_labels_onehot, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 5, 0, 0, 0, 0, 5])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_RNN[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels_RNN = np.argmax(test_labels_onehot, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, texts, lengths, labels):\n",
    "        self.texts = texts\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "        self.size = len(self.texts)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        temp = list(zip(self.texts, self.lengths, self.labels))\n",
    "        random.shuffle(temp)\n",
    "        self.texts, self.lengths, self.labels = zip(*temp)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res_texts = self.texts[self.cursor:self.cursor+n]\n",
    "        res_lengths = self.lengths[self.cursor:self.cursor+n]\n",
    "        res_labels = self.labels[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return res_texts, res_labels, res_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences\n",
      " ([815, 4711, 204, 3711, 446, 2367, 26061, 1245, 742, 2253, 1475, 3759, 13698, 3743, 1853, 636, 68, 2154, 30598, 27479, 1655, 632, 1362, 6615, 40, 1475, 3759, 742, 1362, 1323, 10927, 4914, 5445, 20098, 1655, 1899, 12886, 31526, 13820, 12674, 43840, 4947, 14495, 11695, 3872, 955, 284, 20979, 4186, 24664, 90, 284, 2675, 903, 21, 1878, 53, 6446, 18968, 829, 2244, 31237, 1461, 20536, 15572, 5442, 489, 2584, 316, 1597, 1598, 1790, 5422, 410, 10758, 4531, 2584, 432, 1708, 1121, 1012, 27100, 11678, 117, 7625, 3275, 438, 10513, 4422, 5505, 4102, 2718, 1236, 3188, 29887, 20337, 6551, 1576, 5051, 6788, 1769, 4643, 1769, 4705, 1688, 6788, 1701, 4731, 1078, 11047, 1595, 25146, 14192, 1062, 363, 48, 47, 436, 3223, 472, 1440, 1576, 2218, 523, 7625, 1040, 3125, 3125, 158, 1701, 1615, 458, 2374, 43841, 1063, 23548, 14192, 5211, 3711, 3125, 551, 1790, 12168, 3559, 11047, 3125, 2154, 3125, 23393, 12168, 1944, 10645, 7679, 3125, 18917, 12512, 1475, 3759, 1769, 12168, 1871, 1615, 1266, 7625, 3711, 2584, 1769, 309, 1668, 8803, 5267, 3711, 25, 3711, 779, 3711, 9685, 3711, 3125, 5798, 10423, 22866, 2422, 13590, 18993, 1502, 3711, 2584, 53, 595, 133, 3711, 2584, 2940, 200, 201, 2312, 4889, 20689, 29152, 1797, 48, 2239, 43842, 1266, 4875, 3740, 20689, 14712, 2747, 8370, 5314, 10719, 4167, 2839, 518, 94, 1057, 489, 2265, 4731, 6899, 1769, 144, 14310, 1552, 2625, 1769, 1318, 32, 1152, 2180, 274, 3135, 4268, 518, 11383, 3614, 3070, 268, 2638, 5286, 1720, 15490, 517, 13075, 6788, 284, 489, 6788, 489, 91, 489, 8370, 489, 116, 1701, 1615, 2409, 4638, 26202, 4638, 8656, 8591, 68, 43843, 1475, 3759, 516, 557, 1708, 26202, 6938, 1701, 954, 194, 200, 199, 13460, 13461, 4825, 199, 489, 116, 489, 256, 489, 3786, 1577, 2044, 762, 12863, 1207, 154, 271, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1063, 3118, 1373, 5007, 8902, 1373, 5510, 7023, 269, 3761, 591, 5720, 208, 4372, 19511, 1831, 1373, 28864, 1342, 2248, 1955, 331, 257, 280, 1082, 354, 639, 552, 1487, 639, 5911, 3169, 28864, 1373, 1487, 7270, 3169, 4836, 504, 5085, 4889, 15299, 3369, 37, 301, 1808, 1559, 5493, 3667, 2305, 2561, 15518, 8657, 5829, 1959, 3169, 28864, 1373, 1000, 1955, 1192, 2262, 3956, 4060, 1397, 14, 14502, 186, 257, 280, 79, 632, 1738, 5561, 4542, 840, 688, 1837, 25812, 2237, 6, 2852, 5561, 4542, 4185, 22702, 1959, 3169, 28865, 2761, 3169, 1959, 3169, 1939, 2761, 3169, 865, 1959, 3169, 2280, 2882, 5275, 28866, 16452, 567, 10451, 14401, 1943, 1064, 1955, 1943, 679, 1373, 6022, 1837, 17710, 3294, 2796, 3294, 1637, 5986, 10, 14401, 1959, 3169, 257, 280, 3928, 210, 25678, 280, 472, 437, 3945, 13938, 22335, 3945, 7675, 87, 291, 458, 1959, 14401, 12759, 28867, 6471, 1148, 1017, 4006, 1158, 1143, 6078, 28868, 3493, 2584, 28867, 6078, 4709, 1959, 3169, 28867, 3630, 2761, 3169, 28869, 16452, 19873, 2802, 3851, 12759, 28867, 730, 499, 1837, 12759, 28867, 3630, 679, 2805, 87, 3558, 342, 5262, 20200, 1959, 14401, 4709, 28867, 1959, 3169, 893, 28867, 28869, 1373, 2218, 2796, 5084, 6865, 1637, 5986, 1959, 14401, 134, 28870, 4071, 1943, 142, 142, 7942, 11414, 2761, 2761, 14401, 28871, 545, 208, 987, 437, 5777, 1959, 3169, 1955, 1578, 2761, 3169, 1959, 3169, 24225, 1181, 1005, 873, 1955, 1186, 231, 1955, 1955, 203, 3615, 1959, 3169, 946, 231, 1801, 280, 1959, 3169, 1416, 119, 1955, 1186, 385, 19264, 298, 28869, 16452, 4417, 2218, 1045, 79, 632, 2530, 14401, 639, 742, 1373, 1959, 14401, 3603, 5262, 1934, 28864, 1373, 5007, 3603, 97, 231, 8538, 5720, 208, 1373, 76, 316, 142, 17482, 2406, 3558, 961, 2561, 15518, 8657, 3218, 5911, 2863, 730, 4417, 523, 7530, 976, 993, 1057, 10630, 1373, 5007, 32, 11571, 1722, 1894, 9691, 2307, 8675, 3253, 4845, 142, 3253, 142, 4845, 142, 1082, 7860, 354, 2293, 1326, 6799, 1645, 1440, 16350, 898, 907, 8599, 1504, 707, 6078, 907, 1205, 1943, 3222, 3253, 4355, 1316, 6349, 4438, 1316, 14895, 28872, 3284, 489, 2024, 14196, 11595, 163, 18910, 2484, 437, 28873, 16658, 1365, 489, 2024, 27729, 762, 11507, 3284, 489, 2024, 201, 11507, 1045, 97, 1504, 4836, 1855, 2893, 1332, 3493, 1152, 3281, 59, 4419, 1152, 3493, 2640, 437, 2651, 591, 1943, 1422, 1959, 1422, 2761, 1504, 619, 1959, 2761, 966, 1414, 2305, 1828, 204, 1005, 850, 6078, 129, 204, 1005, 1461, 248, 15238, 1838, 3654, 2162, 10661, 1033, 3654, 3654, 1033, 11527, 2484, 2506, 1160, 436, 79, 1005, 2882, 7884, 1955, 3654, 2484, 117, 3732, 2132, 1005, 1731, 4372, 142, 437, 1839, 4355, 2346, 1213, 5369, 4372, 28864, 1395, 851, 11994, 66, 12886, 426, 214, 2220, 446, 1373, 679, 7339, 437, 1839, 1373, 5720, 14943, 4071, 2616, 8894, 5369, 1160, 2260, 1000, 4836, 504, 2893, 309, 17657, 5821, 9623, 5821, 13718, 5853, 8613, 139, 1914, 8124, 3255, 117, 1099, 13718, 850, 6078, 9623, 5821, 139, 1914, 19848, 2305, 10588, 1504, 12600, 5821, 6078, 1329, 6078, 1544, 1959, 1329, 1000, 5821, 6078, 1544, 2761, 2987, 1604, 1604, 309, 5011, 1440, 1604, 142, 17657, 1696, 985, 1504, 2893, 309, 5011, 1484, 14526, 9854, 1206, 793, 28874, 2503, 156, 985, 1484, 14526, 1206, 793, 153, 16062, 3238, 9258, 14526, 1512, 1369, 7030, 5315, 4621, 8621, 2503, 6708, 28875, 5554, 1024, 5251, 14526, 153, 2977, 1601, 28876, 1236, 26982, 1465, 4303, 854, 381, 2584, 4621, 919, 163, 28877, 1601, 4485, 4059, 28878, 1696, 985, 4485, 2849, 1604, 2849, 1604, 3069, 2977, 1601, 7308, 3672, 985, 1166, 2616, 1604, 2977, 4539, 7308, 3672, 985, 119, 1504, 898, 18910, 2484, 437, 803, 12103, 24310, 21186, 3493, 2920, 8894, 8894, 10020, 115, 28879, 2249, 8373, 595, 2670, 9083, 5084, 3493, 4437, 636, 636, 897, 729, 6984, 10690, 1086, 2220, 1086, 7188, 958, 827, 657, 852, 6981, 21391, 227, 271, 690, 472, 451, 1160, 1086, 2220, 1086, 898, 18910, 437, 7754, 1777, 1361, 208, 3231, 5904, 18229, 1533, 23868, 595, 9245, 3331, 5355, 4896, 1797, 631, 3220, 206, 16057, 3294, 10, 28880, 3220, 1622, 3331, 494, 28881, 160, 8893, 3294, 206, 5217, 1817, 134, 2046, 8812, 5630, 10, 1797, 2533, 628, 1797, 206, 3220, 1166, 869, 4205, 3253, 1205, 14595, 3222, 2108, 3332, 451, 3253, 1720, 7231, 14316, 213, 4789, 1083, 2484, 958, 827, 1720, 7231, 6218, 2150, 2172, 306, 898, 18910, 16809, 1538, 3253, 437, 13710, 1086, 2220, 1086, 3756, 8974, 489, 2024, 2429, 5084, 2429, 868, 3900, 16312, 6444, 3759, 206, 3332, 2108, 3332, 4295, 1017, 1152, 9692, 2484, 128, 167, 2108, 3332, 868, 1017, 2533, 12174, 199, 89, 1797, 8632, 9791, 460, 28251, 2108, 3332, 997, 868, 1152, 5232, 201, 532, 200, 1152, 3719, 3332, 143, 4304, 559, 1152, 28882, 5230, 8024, 892, 24924, 18877, 28883, 2108, 3332, 14196, 5297, 1582, 53, 28884, 907, 28885, 1894, 1797, 639, 5297, 24145, 489, 20094, 5015, 559, 1645, 365, 13932, 1725, 2005, 2898, 2005, 2484, 3650, 898, 1756, 199, 48, 462, 5928, 48, 157, 3301, 48, 28886, 66, 10879, 11598, 3222, 28050, 4438, 2416, 3122, 178, 10772, 5347, 1414, 1648, 18910, 2484, 14913, 2761, 504, 1143, 6760, 3116, 6142, 3582, 2728, 2731, 6151, 2459, 1145, 1533, 813, 2893, 3756, 2651, 3654, 2940, 2305, 11724, 813, 3116, 5115, 1328, 1959, 76, 6329, 591, 1943, 3654, 1422, 2761, 7813, 3294, 7298, 12056, 15826, 12658, 157, 3301, 28886, 6349, 15514, 4438, 1597, 7711, 2305, 1807, 3945, 1894, 218, 219, 883, 7600, 2484, 2605, 28887, 1218, 3603, 3265, 1894, 3332, 5917, 225, 3603, 651, 97, 9256, 1092, 765, 2968, 14459, 3730, 3730, 2888, 1718, 27113, 2747, 10591, 1559, 883, 7600, 2484, 28888, 1053, 17256, 5381, 6530, 8599, 21101, 12567, 6599, 474, 3253, 899, 5911, 966, 3220, 966, 5106, 9083, 517, 2108, 354, 1531, 2182, 4814, 11270, 17903, 5604, 2108, 354, 9256, 3332, 24439, 1152, 1986, 157, 1152, 3301, 157, 3493, 2193, 2108, 1086, 726, 825, 11598, 958, 827, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [35909, 7145, 363, 2020, 194, 27163, 2686, 37457, 134, 7412, 652, 2020, 3240, 2020, 363, 142, 17482, 515, 194, 8776, 14873, 194, 363, 25747, 363, 595, 10108, 2519, 1470, 1193, 306, 1504, 1219, 26537, 7883, 19172, 1385, 11471, 5153, 3601, 1385, 363, 1159, 1111, 7323, 166, 26537, 42, 117, 680, 2228, 350, 10862, 2519, 8967, 24971, 1577, 194, 919, 1330, 982, 1564, 10005, 5199, 4411, 1235, 223, 348, 194, 363, 390, 3666, 366, 194, 363, 10880, 37457, 10, 4194, 37458, 1788, 1153, 498, 143, 1597, 10782, 4560, 2802, 3156, 4889, 2519, 5912, 5006, 37459, 4039, 37460, 14976, 194, 363, 983, 376, 21202, 21202, 9233, 958, 1984, 22, 9233, 2348, 25133, 363, 2695, 792, 1344, 691, 1148, 363, 4180, 1586, 21140, 87, 730, 8666, 3563, 958, 1216, 1029, 730, 28332, 3615, 86, 3156, 2526, 3315, 1218, 1260, 1261, 1262, 2882, 12301, 6017, 14608, 6927, 6462, 5918, 86, 1076, 8589, 3241, 133, 1024, 1775, 21202, 2968, 6268, 1632, 1205, 20529, 4435, 2125, 1218, 156, 726, 730, 6268, 37461, 166, 1167, 480, 10, 18935, 37462, 363, 21202, 1404, 792, 691, 1148, 1404, 1330, 14721, 14721, 4692, 10, 4039, 34051, 2340, 12514, 1040, 6768, 7421, 7597, 28960, 3605, 3603, 3524, 37463, 1329, 7399, 37459, 2771, 5065, 10630, 4039, 37460, 1319, 730, 1237, 87, 5085, 357, 161, 2030, 3109, 16071, 239, 1474, 3612, 16818, 8235, 301, 4401, 270, 825, 161, 13223, 318, 11648, 376, 1522, 4039, 17883, 1115, 16313, 15462, 12808, 3156, 4889, 4573, 10232, 26560, 4815, 1376, 1707, 2812, 4039, 3719, 37459, 1882, 346, 617, 3524, 272, 68, 1597, 23916, 9271, 161, 37459, 134, 194, 6502, 730, 7818, 1100, 3607, 1103, 2030, 2109, 1376, 1060, 2108, 6502, 37464, 1116, 2312, 28321, 3642, 1381, 2623, 37459, 1202, 5828, 2228, 9271, 37465, 2307, 37459, 22957, 5888, 13828, 10226, 15323, 3735, 2374, 4736, 21299, 37466, 37467, 8006, 37468, 19527, 498, 225, 7673, 893, 950, 36285, 15782, 3254, 900, 8236, 3837, 4188, 7931, 4718, 279, 4883, 1212, 363, 9798, 1403, 1989, 160, 17943, 820, 37459, 2086, 8327, 35444, 37469, 830, 1000, 14354, 430, 2711, 24407, 19714, 1449, 37470, 15985, 37470, 21202, 37470, 1505, 1024, 2260, 1779, 1328, 845, 1202, 1985, 1644, 14771, 21202, 363, 19269, 312, 1029, 1788, 9160, 239, 8777, 363, 8777, 10917, 23386, 134, 2048, 4693, 37459, 9803, 458, 21202, 9168, 2170, 24414, 8125, 6857, 9654, 10226, 301, 9709, 363, 545, 21202, 1344, 600, 618, 6782, 779, 11862, 29709, 2125, 3116, 607, 37471, 5902, 203, 4729, 4569, 37472, 4997, 1797, 7775, 37473, 2307, 37474, 4654, 618, 21202, 1613, 318, 13526, 8265, 1815, 37475, 2809, 1844, 618, 37472, 6048, 10267, 7994, 37476, 20691, 608, 13638, 9318, 3956, 2397, 1015, 823, 4134, 5394, 4883, 974, 5394, 10644, 6132, 5176, 5984, 21202, 36098, 10389, 3429, 7240, 5510, 37477, 34879, 1317, 7321, 3605, 1179, 1344, 1207, 22087, 10230, 7328, 1344, 4889, 9954, 2336, 8902, 37478, 9737, 9074, 19296, 385, 37479, 9112, 3855, 844, 8265, 36332, 37480, 12184, 2569, 946, 948, 8349, 2336, 5621, 618, 1473, 5628, 609, 25798, 458, 25798, 1344, 21202, 357, 37459, 37481, 1989, 4696, 7942, 214, 1580, 6707, 214, 308, 4659, 660, 19665, 12070, 211, 20011, 504, 17032, 7642, 158, 6516, 2598, 414, 9798, 1797, 1196, 16706, 298, 21202, 9354, 1918, 25027, 1059, 7994, 1918, 28357, 9112, 9692, 10561, 510, 2228, 3956, 1029, 6645, 7030, 17052, 9318, 710, 2154, 618, 9666, 20011, 37482, 394, 1017, 2025, 1063, 2530, 10947, 21202, 5014, 11347, 1765, 3010, 683, 10046, 974, 3439, 11322, 10046, 5947, 874, 18104, 37483, 298, 4740, 668, 17666, 1989, 6356, 845, 6818, 10925, 10925, 6048, 9940, 14503, 363, 3340, 3323, 3042, 9895, 4281, 25030, 32747, 32, 2011, 11654, 173, 6517, 1644, 1115, 11648, 16313, 6366, 11954, 7585, 1521, 10815, 3090, 235, 8265, 572, 15985, 19263, 21160, 1984, 7627, 37484, 1367, 13850, 161, 2030, 9802, 3352, 2228, 1310, 4039, 7711, 823, 4039, 4188, 18461, 618, 4654, 618, 21202, 8265, 20068, 5780, 1045, 4017, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Target values\n",
      " (5, 0, 0)\n",
      "\n",
      "Sequence lengths\n",
      " (295, 970, 635)\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(train_talk_word_index_exp, train_lenghts, train_labels_RNN)\n",
    "d = data.next_batch(3)\n",
    "print('Input sequences\\n', d[0], end='\\n\\n')\n",
    "print('Target values\\n', d[1], end='\\n\\n')\n",
    "print('Sequence lengths\\n', d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835, 2221)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 2221)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_talk_word_index_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_labels_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_labels_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51593"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = len(vocab),\n",
    "    state_size = 100,\n",
    "    batch_size = 50,\n",
    "    num_classes = 8):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(1.0)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, 2*state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state, dtype=tf.float32)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    #rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "    #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    \n",
    "    last_rnn_output = tf.reduce_mean(rnn_outputs, 1)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def train_graph(graph, batch_size = 50, num_epochs = 10, iterator = SimpleDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        tr = iterator(train_talk_word_index_exp, train_lenghts, train_labels_RNN)\n",
    "        te = iterator(test_talk_word_index_exp, test_lenghts, test_labels_RNN)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f57a7939f98>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "g = build_graph(vocab_size=51594, batch_size=50, num_classes=8, state_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.565405406461 - te: 0.289999999106\n",
      "Accuracy after epoch 2  - tr: 0.569999998642 - te: 0.288000002503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-ea026a546c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-176da7567bbb>\u001b[0m in \u001b[0;36mtrain_graph\u001b[0;34m(graph, batch_size, num_epochs, iterator)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seqlen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0maccuracy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scratch/ms16lg2/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses = train_graph(g, batch_size = 50, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs have an advantage because for the words are inputted in a sequence there for there is a possiblity that the NN can actually infer something from the sequence rather than just the bag of words representation which completelly ignores order and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the last output offers no significant improvement.b"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
